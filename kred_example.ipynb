{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoboTuan/KRED/blob/master/kred_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr7Xm97Nmuh7",
        "outputId": "f6a480e4-4970-4652-b078-66cf459980fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  1 20:39:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "e6eKhdNlJanS"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1OQ-2ohJanX"
      },
      "source": [
        "This repository is the implementation of [KRED: Knowledge-Aware Document Representation for News Recommendations](https://arxiv.org/abs/1910.11494) [1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oqUeUvJanY"
      },
      "source": [
        "## Model description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nfWkEvNJanZ"
      },
      "source": [
        "\n",
        "\n",
        "KRED is a knowledge enhanced framework which enhance a document embedding with knowledge information for multiple news recommendation tasks. The framework mainly contains two part: representation enhancement part(left) and multi-task training part(right)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRsGOwjoJana"
      },
      "source": [
        "![](./framework.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7P8rDKfJana"
      },
      "source": [
        "## Dataset description and download\n",
        "\n",
        "MIND dataset [2] is a large-scale English news dataset. It was collected from anonymized behavior logs of Microsoft News website. MIND contains 1,000,000 users, 161,013 news articles and 15,777,377 impression logs. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression.\n",
        "\n",
        "For quicker training and evaluaiton, we sample MINDdemo dataset of 5k users from MIND small dataset. The MINDdemo dataset has the same file format as MINDsmall and MINDlarge. If you want to try experiments on MINDsmall and MINDlarge, please change the dowload source. Select the MIND_type parameter from ['large', 'small', 'demo'] to choose dataset.\n",
        "\n",
        "MINDdemo_train is used for training, and MINDdemo_dev is used for evaluation. Training data and evaluation data are composed of a news file and a behaviors file. You can find more detailed data description in [MIND repo](https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yneOM1lr1ZJ3",
        "outputId": "d76a9db1-52b9-4cd3-f97a-81fb4ea9c304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uO6afb_LpeF",
        "outputId": "7858d3e8-4b08-4593-d0fa-5df9455ccc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KRED'...\n",
            "remote: Enumerating objects: 292, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 292 (delta 108), reused 119 (delta 88), pack-reused 139\u001b[K\n",
            "Receiving objects: 100% (292/292), 44.16 MiB | 14.44 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.isdir('KRED'):\n",
        "  !git clone https://github.com/RoboTuan/KRED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gaxJejGp1mjc"
      },
      "outputs": [],
      "source": [
        "!mkdir ./data\n",
        "!mkdir ./data/train\n",
        "!mkdir ./data/valid\n",
        "!mkdir ./data/kg\n",
        "!cp /content/drive/MyDrive/MINDsmall_dev.zip ./data/valid\n",
        "!cp /content/drive/MyDrive/MINDsmall_train.zip ./data/train\n",
        "!cp /content/drive/MyDrive/kg.zip ./data/kg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OViEAbCuOj-M",
        "outputId": "f74cf4d3-42ad-4934-9c02-0c15cd4b87fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=8cd703b40e8992cebd32b43b174129ca585860a8319829b97ac7d93635a98522\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence-transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vOmDptHE1N4v"
      },
      "outputs": [],
      "source": [
        "!cp KRED/config.yaml ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E-neBIBbJanb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('KRED')\n",
        "import os\n",
        "from utils.util import *\n",
        "from train_test import *\n",
        "\n",
        "# Options: demo, small, large\n",
        "MIND_type = 'small'\n",
        "data_path = \"./data/\"\n",
        "\n",
        "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
        "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
        "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
        "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
        "knowledge_graph_file = os.path.join(data_path, 'kg/wikidata-graph', r'wikidata-graph.tsv')\n",
        "entity_embedding_file = os.path.join(data_path, 'kg/wikidata-graph', r'entity2vecd100.vec')\n",
        "relation_embedding_file = os.path.join(data_path, 'kg/wikidata-graph', r'relation2vecd100.vec')\n",
        "\n",
        "mind_url, mind_train_dataset, mind_dev_dataset, _ = get_mind_data_set(MIND_type)\n",
        "\n",
        "kg_url = \"https://kredkg.blob.core.windows.net/wikidatakg/\"\n",
        "\n",
        "if not os.path.exists(train_news_file):\n",
        "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
        "    \n",
        "if not os.path.exists(valid_news_file):\n",
        "    download_deeprec_resources(mind_url, \\\n",
        "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
        "\n",
        "if not os.path.exists(knowledge_graph_file):\n",
        "    download_deeprec_resources(kg_url, \\\n",
        "                               os.path.join(data_path, 'kg'), \"kg.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run2vFbRJand"
      },
      "source": [
        "## loading config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cho7N7eIJane",
        "outputId": "af15f337-1ad8-4ac0-9bb3-1e633cc4da9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: logging configuration file is not found in logger/logger_config.json.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<parse_config.ConfigParser at 0x7ff4aabddd50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('')\n",
        "sys.argv = ['']\n",
        "\n",
        "import argparse\n",
        "from parse_config import ConfigParser\n",
        "\n",
        "parser = argparse.ArgumentParser(description='KRED')\n",
        "\n",
        "\n",
        "parser.add_argument('-c', '--config', default=\"./KRED/config.yaml\", type=str,\n",
        "                    help='config file path (default: None)')\n",
        "parser.add_argument('-r', '--resume', default=None, type=str,\n",
        "                    help='path to latest checkpoint (default: None)')\n",
        "parser.add_argument('-d', '--device', default=None, type=str,\n",
        "                    help='indices of GPUs to enable (default: all)')\n",
        "\n",
        "#config = parser.parse_args(\"\")\n",
        "config = ConfigParser.from_args(parser)\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DPwoO1EJanf"
      },
      "source": [
        "## Create hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2cn32UdXJang"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "batch_size = 64\n",
        "train_type = \"single_task\"\n",
        "task = \"user2item\" # task should be within: user2item, item2item, vert_classify, pop_predict\n",
        "\n",
        "config['trainer']['epochs'] = epochs\n",
        "config['data_loader']['batch_size'] = batch_size\n",
        "config['trainer']['training_type'] = train_type\n",
        "config['trainer']['task'] = task\n",
        "config['trainer']['save_period'] = epochs/2\n",
        "config['data']['sentence_embedding_folder'] = \"/content/drive/MyDrive/sentence_embedding/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56wYPEAdJanh"
      },
      "source": [
        "## Process dataset\n",
        "\n",
        "Since MIND dataset do not contain user's location information, we can not use local news \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mvpLOm1Uqnvk"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(\"/content/drive/MyDrive/sentence_embedding/train_news_embeddings.pkl\"):\n",
        "    write_embedding_news(\"./data/train\", config[\"data\"][\"sentence_embedding_folder\"])\n",
        "\n",
        "if not os.path.isfile(\"/content/drive/MyDrive/sentence_embedding/valid_news_embeddings.pkl\"):\n",
        "    write_embedding_news(\"./data/valid\", config[\"data\"][\"sentence_embedding_folder\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hhEzH0ZCvY1j"
      },
      "outputs": [],
      "source": [
        "# data = load_data_mind(config, sentence_embedding_folder)\n",
        "if not os.path.isfile(\"/content/drive/MyDrive/sentence_embedding/data_mind.pkl\"):\n",
        "    write_data_mind(config, \"/content/drive/MyDrive/sentence_embedding/\")\n",
        "data = read_pickle(\"/content/drive/MyDrive/sentence_embedding/data_mind.pkl\")\n",
        "\n",
        "test_data = data[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E5azg6Bp8Rpw"
      },
      "outputs": [],
      "source": [
        "def limit_user2item_validation_data(data, size):\n",
        "    test_data = data[-1]\n",
        "    test_data_reduced = {key: test_data[key][:size] for key in test_data.keys()}\n",
        "    # Concatenate the old tuple with the updated validation data\n",
        "    return data[:-1] + (test_data_reduced,)\n",
        "\n",
        "data = limit_user2item_validation_data(data, 10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4PmTj18vPn",
        "outputId": "33fd5c87-85ce-4eef-971e-ee0a41448828"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA24LhHVJanj"
      },
      "source": [
        "## Train the KRED model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvBZu0aCnEqV",
        "outputId": "d4b5cbf6-54ea-4b6e-b294-fe273302f315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:train:model training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all loss: tensor(1707.3665, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:trainer:Saving checkpoint: out/saved/models/KRED/0501_210029/checkpoint-model-epoch1.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc socre: 0.6016132115953025\n"
          ]
        }
      ],
      "source": [
        "single_task_training(config, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld5wSMjmJanl"
      },
      "source": [
        "## Evaluate the KRED model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMWISudxnC9K",
        "outputId": "24c32325-7baf-44db-9440-cb7677f25432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc score:0.6149325123656655\n",
            "ndcg score:0.3369811877204074\n"
          ]
        }
      ],
      "source": [
        "testing(test_data, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKPWdlGJann"
      },
      "source": [
        "## Performance on MINDlarge\n",
        "\n",
        "we test the performance on MINDlarge dev dataset for your reference:\n",
        "\n",
        "| Models | AUC | NDCG@10 |\n",
        "| :------- | :------- | :------- |\n",
        "| KRED(single task training) | 0.6702 | 0.4018 |\n",
        "| KRED(multi task training) |  0.6731 | 0.4039|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9vsdpqvJann"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksa15kWJJano"
      },
      "source": [
        "[1] Liu, Danyang, et al. \"KRED: Knowledge-Aware Document Representation for News Recommendations.\" Fourteenth ACM Conference on Recommender Systems. 2020.\n",
        "\n",
        "[2] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}